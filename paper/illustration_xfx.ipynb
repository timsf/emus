{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.special import logsumexp\n",
    "from scipy.stats import norm, multivariate_normal, gaussian_kde\n",
    "\n",
    "from umbrella import emus\n",
    "from umbrella.models import xfx\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config \n",
    "\n",
    "n_windows = 16\n",
    "n_samples = 2\n",
    "n_burnin = 2\n",
    "n_levels = np.array([16, 16])\n",
    "alp0 = 0\n",
    "tau0 = 1\n",
    "lam = 1\n",
    "df_tau = 1e64\n",
    "scale_tau = 1e-64\n",
    "seed = 0\n",
    "bounds1 = [-1, 1]\n",
    "bounds2 = [0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design\n",
    "\n",
    "def sample_coef_fixture(j, tau, ome):\n",
    "    alp = [ome.normal(0, 1 / np.sqrt(tau_), j_) for tau_, j_ in zip(tau, j)]\n",
    "    return [alp_ - np.mean(alp_) for alp_ in alp]\n",
    "\n",
    "def sample_randfx_fixture(i, df_tau, scale_tau, ome):\n",
    "    tau = scale_tau * ome.chisquare(df_tau, len(i))\n",
    "    alp = sample_coef_fixture(i, tau, ome)\n",
    "    return alp, tau\n",
    "\n",
    "def sample_data_fixture(i, n_inflator, alp0, alp, lam, ome):\n",
    "    y = ome.normal((alp0 + np.sum([alp_[j_] for alp_, j_ in zip(alp, i.T)], 0))[:, np.newaxis],\n",
    "                   1 / np.sqrt(lam), size=(i.shape[0], n_inflator))\n",
    "    y1 = np.sum(y, 1)\n",
    "    y2 = np.sum(np.square(y), 1)\n",
    "    n = np.repeat(n_inflator, i.shape[0])\n",
    "    return y1, y2, n\n",
    "\n",
    "def sample_balanced_design(j, ome):\n",
    "    n = 1\n",
    "    for j_ in j:\n",
    "        n = np.lcm(n, j_)\n",
    "    i = np.array([np.repeat(np.arange(j_), n / j_) for j_ in j]).T\n",
    "    for k_ in range(len(j)):\n",
    "        ome.shuffle(i[:, k_])\n",
    "    return i\n",
    "\n",
    "def sample_balanced_fixture(j, alp0=0, lam=1, df_tau=2, scale_tau=1, n_inflator=1, ome=np.random.default_rng()):\n",
    "    alp, tau = sample_randfx_fixture(j, df_tau, scale_tau, ome)\n",
    "    i = sample_balanced_design(j, ome)\n",
    "    y1, y2, n = sample_data_fixture(i, n_inflator, alp0, alp, lam, ome)\n",
    "    return (y1, y2, n, j, i), (alp0, alp, tau, lam)\n",
    "\n",
    "def sample_mar_design(j, p_miss, ome):\n",
    "    i = np.stack(np.meshgrid(*[np.arange(j_) for j_ in j])).T.reshape(-1, 2)\n",
    "    i = i[ome.uniform(size=i.shape[0]) > p_miss]\n",
    "    ome.shuffle(i, 0)\n",
    "    return i\n",
    "\n",
    "def sample_mar_fixture(j, df_tau=2, scale_tau=1, p_miss=.1, ome=np.random.default_rng()):\n",
    "    alp0 = 0\n",
    "    alp, tau = sample_randfx_fixture(j, df_tau, scale_tau, ome)\n",
    "    i = sample_mar_design(j, p_miss, ome)\n",
    "    eta = alp0 + np.sum([alp_[j_] for alp_, j_ in zip(alp, i.T)], 0)\n",
    "    return (eta, i), (alp0, alp, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling\n",
    "\n",
    "def eval_logmargin(y1, n, j, i, tau0, tau, lam):\n",
    "    s22 = np.diag(1 / (lam * n)) + 1 / tau0\n",
    "    for k_ in range(len(j)):\n",
    "        for j_ in range(j[k_]):\n",
    "            s22[np.ix_(i[:, k_] == j_, i[:, k_] == j_)] += 1 / tau[k_]\n",
    "    return multivariate_normal.logpdf(y1 / n, np.zeros_like(y1), s22) \n",
    "\n",
    "def eval_logprior(alp0, alp, tau0, tau):\n",
    "    log_prior = np.sum([-(len(alp_) * np.log(2 * np.pi / tau_) + tau_ * np.sum(np.square(alp_))) / 2 for alp_, tau_ in zip([[alp0]] + alp, [tau0] + list(tau))])\n",
    "    return log_prior\n",
    "\n",
    "def sample_cond(w1, w2, x0, x1, x2, j, tau0, tau, lam, ome):\n",
    "    alp0 = 0\n",
    "    alp = [np.zeros(j_) for j_ in j]\n",
    "    while True:\n",
    "        alp = xfx.update_coefs(x1, x2, None, alp, tau0, tau, lam, ome)\n",
    "        alp0 = xfx.update_intercept(x0, x1, alp, tau0, lam, ome)\n",
    "        log_prior1 = [eval_logprior(alp0, alp, tau0, tau_) for tau_ in w1]\n",
    "        log_prior2 = [eval_logprior(alp0, alp, tau0, tau_) for tau_ in w2]\n",
    "        yield np.array(log_prior1), np.array(log_prior2)\n",
    "\n",
    "def sample_joint(w1, x0, x1, x2, j, tau0, lam, ome):\n",
    "    alp0 = 0\n",
    "    alp = [np.zeros(j_) for j_ in j]\n",
    "    idx = ome.choice(len(w1))\n",
    "    tau = w1[idx]\n",
    "    while True:\n",
    "        alp = xfx.update_coefs(x1, x2, None, alp, tau0, tau, lam, ome)\n",
    "        alp0 = xfx.update_intercept(x0, x1, alp, tau0, lam, ome)\n",
    "        log_prior = [eval_logprior(alp0, alp, tau0, tau_) for tau_ in w1]\n",
    "        idx = ome.choice(len(w1), p=np.exp(np.array(log_prior) - logsumexp(log_prior)))\n",
    "        tau = w1[idx]\n",
    "        yield idx\n",
    "    \n",
    "def est_margin_emus(w1, w2, y1, y2, n, j, i, tau0, lam, ome, n_samples, n_burnin):\n",
    "    x0, x1, x2 = xfx.reduce_data(y1, y2, n, i)\n",
    "    samplers = [sample_cond(w1, w2, x0, x1, x2, j, tau0, w_, lam, ome) for w_ in w1]\n",
    "    for sampler in samplers:\n",
    "        for _ in zip(range(n_burnin), sampler):\n",
    "            continue\n",
    "    log_psi = [list(zip(range(n_samples), sampler)) for sampler in samplers]\n",
    "    log_psi1 = [np.array([log_psi1__ for _, (log_psi1__, _) in log_psi_]) for log_psi_ in log_psi]\n",
    "    log_psi2 = [np.array([log_psi2__ for _, (_, log_psi2__) in log_psi_]) for log_psi_ in log_psi]\n",
    "    z1 = emus.eval_vardi_estimator(log_psi1)[0]\n",
    "    z2 = emus.extrapolate(log_psi2, log_psi1, z1)\n",
    "    return z2 / np.sum(z2)\n",
    "    \n",
    "def est_margin_griddy(w1, w2, y1, y2, n, j, i, tau0, lam, ome, n_samples, n_burnin):\n",
    "    x0, x1, x2 = xfx.reduce_data(y1, y2, n, i)\n",
    "    sampler = sample_joint(w1, x0, x1, x2, j, tau0, lam, ome)\n",
    "    for _, idx_ in zip(range(n_burnin * len(w1)), sampler):\n",
    "        continue\n",
    "    idx = [idx_]\n",
    "    for _, idx_ in zip(range(n_samples * len(w1)), sampler):\n",
    "        idx.append(idx_)\n",
    "    z2 = gaussian_kde(np.log(np.array(w1)[idx].T))(np.log(np.array(w2).T))\n",
    "    return z2 / np.sum(z2)\n",
    "    #return np.bincount(idx, minlength=len(w1)) / (n_samples * len(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data\n",
    "\n",
    "ome = np.random.default_rng(seed)\n",
    "data, params = sample_mar_fixture(n_levels, df_tau, scale_tau, 0, ome)\n",
    "y1 = ome.normal(data[0], np.sqrt(1/lam))\n",
    "#data = (y1, np.square(y1), np.ones_like(y1), n_levels, data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate grid# generate grid\n",
    "\n",
    "wa1 = np.exp(np.linspace(*bounds1, n_windows))\n",
    "wb1 = np.exp(np.linspace(*bounds2, n_windows))\n",
    "w1 = [np.array([lam1_, lam2_]) for (lam1_, lam2_) in product(wa1, wb1)]\n",
    "\n",
    "wa2 = np.exp(np.linspace(*bounds1, n_windows * 2))\n",
    "wb2 = np.exp(np.linspace(*bounds2, n_windows * 2))\n",
    "w2 = [np.array([lam1_, lam2_]) for (lam1_, lam2_) in product(wa2, wb2)]\n",
    "\n",
    "x_rec, y_rec = np.meshgrid(wa2, wb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate ground truth\n",
    "\n",
    "log_z = [eval_logmargin(y1, np.ones_like(y1), n_levels, data[1], tau0, w_, lam) for w_ in w2]\n",
    "z = np.exp(np.array(log_z) - logsumexp(log_z))\n",
    "z_rec = np.reshape(z, 2 * (int(np.sqrt(len(z))),))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.contourf(x_rec, y_rec, z_rec, 8, cmap='magma')\n",
    "ax2.pcolormesh(x_rec, y_rec, z_rec, cmap='magma')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax1.set_xlabel(r'$\\lambda_{1}$')\n",
    "ax1.set_ylabel(r'$\\lambda_{2}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate by emus\n",
    "\n",
    "z_emus = est_margin_emus(w1, w2, y1, y1 ** 2, np.ones_like(y1), n_levels, data[1], tau0, lam, ome, n_samples, n_burnin)\n",
    "z_rec_emus = np.reshape(z_emus, 2 * (int(np.sqrt(len(z_emus))),))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.contourf(x_rec, y_rec, z_rec_emus, 8, cmap='magma')\n",
    "ax2.pcolormesh(x_rec, y_rec, z_rec_emus, cmap='magma')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax1.set_xlabel(r'$\\lambda_{1}$')\n",
    "ax1.set_ylabel(r'$\\lambda_{2}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimate by griddy\n",
    "\n",
    "z_griddy = est_margin_griddy(w1, w2, y1, y1 ** 2, np.ones_like(y1), n_levels, data[1], tau0, lam, ome, n_samples, 2 * n_burnin)\n",
    "z_rec_griddy = np.reshape(z_griddy, 2 * (int(np.sqrt(len(z_griddy))),))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax1.contourf(x_rec, y_rec, z_rec_griddy, 8, cmap='magma')\n",
    "ax2.pcolormesh(x_rec, y_rec, z_rec_griddy, cmap='magma')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax1.set_xlabel(r'$\\lambda_{1}$')\n",
    "ax1.set_ylabel(r'$\\lambda_{2}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error distribution of respective methods\n",
    "\n",
    "n_sims = 16\n",
    "z_emus_sim = np.array([est_margin_emus(w1, w2, y1, y1 ** 2, np.ones_like(y1), n_levels, data[1], tau0, lam, ome, n_samples, n_burnin) for _ in range(n_sims)])\n",
    "z_griddy_sim = np.array([est_margin_griddy(w1, w2, y1, y1 ** 2, np.ones_like(y1), n_levels, data[1], tau0, lam, ome, n_samples, n_burnin) for _ in range(n_sims)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# marginal error plots\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ax1.plot(wa2, np.sum(z_rec, 0), color='black')\n",
    "for z_marg1 in [np.sum(np.reshape(z_, 2 * (int(np.sqrt(len(z_))),)), 0) for z_ in z_emus_sim]:\n",
    "    ax1.plot(wa2, z_marg1, color=sns.color_palette('colorblind')[0], alpha=.25)\n",
    "for z_marg1 in [np.sum(np.reshape(z_, 2 * (int(np.sqrt(len(z_))),)), 0) for z_ in z_griddy_sim]:\n",
    "    ax1.plot(wa2, z_marg1, color=sns.color_palette('colorblind')[1], alpha=.25)\n",
    "ax1.set_xlabel(r'$\\lambda_{1}$')\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "ax2.plot(wb2, np.sum(z_rec, 1), color='black')\n",
    "for z_marg1 in [np.sum(np.reshape(z_, 2 * (int(np.sqrt(len(z_))),)), 1) for z_ in z_emus_sim]:\n",
    "    ax2.plot(wb2, z_marg1, color=sns.color_palette('colorblind')[0], alpha=.25)\n",
    "for z_marg1 in [np.sum(np.reshape(z_, 2 * (int(np.sqrt(len(z_))),)), 1) for z_ in z_griddy_sim]:\n",
    "    ax2.plot(wb2, z_marg1, color=sns.color_palette('colorblind')[1], alpha=.25)\n",
    "ax2.set_xlabel(r'$\\lambda_{2}$')\n",
    "ax2.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# profile error plots\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "ax1.plot(wa2, np.max(z_rec, 0), color='black')\n",
    "for z_marg1 in [np.max(np.reshape(z_, 2 * (int(np.sqrt(len(z_))),)), 0) for z_ in z_emus_sim]:\n",
    "    ax1.plot(wa2, z_marg1, color=sns.color_palette('colorblind')[0], alpha=.25)\n",
    "for z_marg1 in [np.max(np.reshape(z_, 2 * (int(np.sqrt(len(z_))),)), 0) for z_ in z_griddy_sim]:\n",
    "    ax1.plot(wa2, z_marg1, color=sns.color_palette('colorblind')[1], alpha=.25)\n",
    "ax1.set_xlabel(r'$\\lambda_{1}$')\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "ax2.plot(wb2, np.max(z_rec, 1), color='black')\n",
    "for z_marg1 in [np.max(np.reshape(z_, 2 * (int(np.sqrt(len(z_))),)), 1) for z_ in z_emus_sim]:\n",
    "    ax2.plot(wb2, z_marg1, color=sns.color_palette('colorblind')[0], alpha=.25)\n",
    "for z_marg1 in [np.max(np.reshape(z_, 2 * (int(np.sqrt(len(z_))),)), 1) for z_ in z_griddy_sim]:\n",
    "    ax2.plot(wb2, z_marg1, color=sns.color_palette('colorblind')[1], alpha=.25)\n",
    "ax2.set_xlabel(r'$\\lambda_{2}$')\n",
    "ax2.set_xscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=pd.DataFrame({'emus': np.linalg.norm(z_emus_sim - z, 1, 1), 'griddy': np.linalg.norm(z_griddy_sim - z, 1, 1)}).melt(var_name='algo'), x='algo', y='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umbrella-OU96uE9v-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
